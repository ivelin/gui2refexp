{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ivelin/gui2refexp/blob/main/dataset/tfrecord_export.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Checkout source files from github repo\n",
        "![[ -d \"gui2refexp\" ]] || git clone https://github.com/ivelin/gui2refexp.git \n",
        "\n",
        "!cd gui2refexp && git pull\n"
      ],
      "metadata": {
        "id": "YMpB40An2J7a",
        "outputId": "b6866ff7-75a0-45a4-9e4f-8ede2dfc6c4b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'gui2refexp'...\n",
            "remote: Enumerating objects: 131, done.\u001b[K\n",
            "remote: Counting objects: 100% (131/131), done.\u001b[K\n",
            "remote: Compressing objects: 100% (98/98), done.\u001b[K\n",
            "remote: Total 131 (delta 44), reused 106 (delta 29), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (131/131), 8.58 MiB | 11.21 MiB/s, done.\n",
            "Resolving deltas: 100% (44/44), done.\n",
            "Already up to date.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pL--_KGdYoBz"
      },
      "source": [
        "##### Copyright 2019 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "uBDvXpYzYnGj"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pkUd_9IZCFO"
      },
      "source": [
        "The TFRecord format is a simple format for storing a sequence of binary records.\n",
        "\n",
        "[Protocol buffers](https://developers.google.com/protocol-buffers/) are a cross-platform, cross-language library for efficient serialization of structured data.\n",
        "\n",
        "Protocol messages are defined by `.proto` files, these are often the easiest way to understand a message type.\n",
        "\n",
        "The `tf.train.Example` message (or protobuf) is a flexible message type that represents a `{\"string\": value}` mapping. It is designed for use with TensorFlow and is used throughout the higher-level APIs such as [TFX](https://www.tensorflow.org/tfx/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ac83J0QxjhFt"
      },
      "source": [
        "This notebook demonstrates how to create, parse, and use the `tf.train.Example` message, and then serialize, write, and read `tf.train.Example` messages to and from `.tfrecord` files.\n",
        "\n",
        "Note: While useful, these structures are optional. There is no need to convert existing code to use TFRecords, unless you are [using tf.data](https://www.tensorflow.org/guide/data) and reading data is still the bottleneck to training. You can refer to [Better performance with the tf.data API](https://www.tensorflow.org/guide/data_performance) for dataset performance tips.\n",
        "\n",
        "Note: In general, you should shard your data across multiple files so that you can parallelize I/O (within a single host or across multiple hosts). The rule of thumb is to have at least 10 times as many files as there will be hosts reading data. At the same time, each file should be large enough (at least 10 MB+ and ideally 100 MB+) so that you can benefit from I/O prefetching. For example, say you have `X` GB of data and you plan to train on up to `N` hosts. Ideally, you should shard the data to ~`10*N` files, as long as ~`X/(10*N)` is 10 MB+ (and ideally 100 MB+). If it is less than that, you might need to create fewer shards to trade off parallelism benefits and I/O prefetching benefits."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkRreBf1eDVc"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Ja7sezsmnXph"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import IPython.display as display"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6qxofy89obI"
      },
      "source": [
        "## TFRecords format details\n",
        "\n",
        "A TFRecord file contains a sequence of records. The file can only be read sequentially.\n",
        "\n",
        "Each record contains a byte-string, for the data-payload, plus the data-length, and  CRC-32C ([32-bit CRC](https://en.wikipedia.org/wiki/Cyclic_redundancy_check#CRC-32_algorithm) using the [Castagnoli polynomial](https://en.wikipedia.org/wiki/Cyclic_redundancy_check#Standards_and_common_use)) hashes for integrity checking.\n",
        "\n",
        "Each record is stored in the following formats:\n",
        "\n",
        "    uint64 length\n",
        "    uint32 masked_crc32_of_length\n",
        "    byte   data[length]\n",
        "    uint32 masked_crc32_of_data\n",
        "\n",
        "The records are concatenated together to produce the file. CRCs are\n",
        "[described here](https://en.wikipedia.org/wiki/Cyclic_redundancy_check), and\n",
        "the mask of a CRC is:\n",
        "\n",
        "    masked_crc = ((crc >> 15) | (crc << 17)) + 0xa282ead8ul\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyg1g3gU7DNn"
      },
      "source": [
        "## TFRecord files in Python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FXG3miA7Kf1"
      },
      "source": [
        "The `tf.io` module also contains pure-Python functions for reading and writing TFRecord files."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2osVRnYNni-E"
      },
      "source": [
        "### Reading a TFRecord file\n",
        "\n",
        "These serialized tensors can be easily parsed using `tf.train.Example.ParseFromString`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "U3tnd3LerOtV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77e572ec-cb1b-4992-bcf2-f47d5aa4ecc2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TFRecordDatasetV2 element_spec=TensorSpec(shape=(), dtype=tf.string, name=None)>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "filenames = [\"gui2refexp/dataset/pix2struct_data_data_refexp_test.tfrecord\"]\n",
        "raw_dataset = tf.data.TFRecordDataset(filenames)\n",
        "raw_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "nsEAACHcnm3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee38d833-17ae-44a2-8461-b057d121c47b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total records in the dataset: 616\n",
            "---------------------------------------------\n",
            "image/object/num float_list {\n",
            "  value: 32.0\n",
            "}\n",
            "\n",
            "image/id bytes_list {\n",
            "  value: \"61654\"\n",
            "}\n",
            "\n",
            "image/object/bbox/xmin\n",
            "image/view_hierarchy/bbox/ymin\n",
            "image/object/bbox/ymin\n",
            "image/ref_exp/text bytes_list {\n",
            "  value: \"select the left side bottom image\"\n",
            "}\n",
            "\n",
            "image/view_hierarchy/bbox/xmax\n",
            "image/view_hierarchy/class/name\n",
            "image/view_hierarchy/id/name\n",
            "image/view_hierarchy/text bytes_list {\n",
            "  value: \"\"\n",
            "  value: \"tv shows\"\n",
            "  value: \"\"\n",
            "  value: \"\"\n",
            "  value: \"popular\"\n",
            "  value: \"recently added\"\n",
            "  value: \"\"\n",
            "  value: \"i dont watch tv\"\n",
            "  value: \" views\"\n",
            "  value: \"\"\n",
            "  value: \"ranjish\"\n",
            "  value: \" views\"\n",
            "  value: \"\"\n",
            "  value: \"\"\n",
            "  value: \"\"\n",
            "  value: \"are ho ja re gender\"\n",
            "  value: \"\"\n",
            "  value: \"\"\n",
            "  value: \"john smith\"\n",
            "  value: \"appcrawler4 gmail com\"\n",
            "  value: \"\"\n",
            "  value: \"\"\n",
            "  value: \"\"\n",
            "  value: \"home\"\n",
            "  value: \"\"\n",
            "  value: \"live tv\"\n",
            "  value: \"\"\n",
            "  value: \"catch up tv\"\n",
            "  value: \"\"\n",
            "  value: \"tv shows\"\n",
            "  value: \"\"\n",
            "  value: \"yuppflix\"\n",
            "  value: \"\"\n",
            "  value: \"pay per view\"\n",
            "  value: \"\"\n",
            "  value: \"bazaar\"\n",
            "  value: \"\"\n",
            "  value: \"news\"\n",
            "  value: \"\"\n",
            "  value: \"recently watched\"\n",
            "  value: \"\"\n",
            "  value: \"favourites\"\n",
            "  value: \"\"\n",
            "  value: \"tv guide\"\n",
            "  value: \"\"\n",
            "  value: \"packages\"\n",
            "  value: \"\"\n",
            "  value: \"language s\"\n",
            "  value: \"\"\n",
            "  value: \"\"\n",
            "}\n",
            "\n",
            "image/view_hierarchy/description bytes_list {\n",
            "  value: \"open drawer navigation\"\n",
            "  value: \"\"\n",
            "  value: \"search\"\n",
            "  value: \"play on\"\n",
            "  value: \"\"\n",
            "  value: \"\"\n",
            "  value: \"\"\n",
            "  value: \"\"\n",
            "  value: \"\"\n",
            "  value: \"\"\n",
            "  value: \"\"\n",
            "  value: \"\"\n",
            "  value: \"\"\n",
            "  value: \"\"\n",
            "  value: \"\"\n",
            "  value: \"\"\n",
            "  value: \"\"\n",
            "  value: \"yupptv\"\n",
            "  value: \"\"\n",
            "  value: \"\"\n",
            "  value: \"\"\n",
            "  value: \"\"\n",
            "  value: \"\"\n",
            "  value: \"\"\n",
            "  value: \"\"\n",
            "  value: \"\"\n",
            "  value: \"\"\n",
            "  value: \"\"\n",
            "  value: \"\"\n",
            "  value: \"\"\n",
            "  value: \"\"\n",
            "  value: \"\"\n",
            "  value: \"\"\n",
            "  value: \"\"\n",
            "  value: \"\"\n",
            "  value: \"\"\n",
            "  value: \"\"\n",
            "  value: \"\"\n",
            "  value: \"\"\n",
            "  value: \"\"\n",
            "  value: \"\"\n",
            "  value: \"\"\n",
            "  value: \"\"\n",
            "  value: \"\"\n",
            "  value: \"\"\n",
            "  value: \"\"\n",
            "  value: \"\"\n",
            "  value: \"\"\n",
            "  value: \"\"\n",
            "  value: \"\"\n",
            "}\n",
            "\n",
            "image/view_hierarchy/bbox/xmin\n",
            "image/object/bbox/ymax\n",
            "image/object/bbox/xmax\n",
            "image/view_hierarchy/class/label\n",
            "image/ref_exp/label\n",
            "image/view_hierarchy/bbox/ymax\n",
            "---------------------------------------------\n",
            "image/view_hierarchy/class/name\n",
            "image/view_hierarchy/description bytes_list {\n",
            "  value: \"navigate up\"\n",
            "  value: \"\"\n",
            "  value: \"abbreviations\"\n",
            "  value: \"more options\"\n",
            "  value: \"\"\n",
            "  value: \"\"\n",
            "  value: \"\"\n",
            "  value: \"\"\n",
            "  value: \"\"\n",
            "  value: \"sinhala dictionary\"\n",
            "  value: \"sinhala dictionary\"\n",
            "  value: \"\"\n",
            "  value: \"\"\n",
            "  value: \"\"\n",
            "  value: \"\"\n",
            "}\n",
            "\n",
            "image/object/bbox/ymin\n",
            "image/view_hierarchy/text bytes_list {\n",
            "  value: \"\"\n",
            "  value: \"search results\"\n",
            "  value: \"\"\n",
            "  value: \"\"\n",
            "  value: \"\"\n",
            "  value: \" \"\n",
            "  value: \"\"\n",
            "  value: \"phy\"\n",
            "  value: \"orange\"\n",
            "  value: \"\"\n",
            "  value: \"\"\n",
            "  value: \"\"\n",
            "  value: \"\"\n",
            "  value: \"\"\n",
            "  value: \"\"\n",
            "}\n",
            "\n",
            "image/object/bbox/ymax\n",
            "image/ref_exp/label\n",
            "image/view_hierarchy/bbox/xmin\n",
            "image/id bytes_list {\n",
            "  value: \"35617\"\n",
            "}\n",
            "\n",
            "image/view_hierarchy/class/label\n",
            "image/object/bbox/xmin\n",
            "image/view_hierarchy/bbox/ymin\n",
            "image/view_hierarchy/bbox/xmax\n",
            "image/view_hierarchy/id/name\n",
            "image/object/bbox/xmax\n",
            "image/object/num float_list {\n",
            "  value: 9.0\n",
            "}\n",
            "\n",
            "image/view_hierarchy/bbox/ymax\n",
            "image/ref_exp/text bytes_list {\n",
            "  value: \"select the blue icon\"\n",
            "}\n",
            "\n",
            "---------------------------------------------\n",
            "image/ref_exp/label\n",
            "image/object/bbox/xmin\n",
            "image/view_hierarchy/bbox/xmax\n",
            "image/object/bbox/ymin\n",
            "image/view_hierarchy/bbox/ymin\n",
            "image/view_hierarchy/class/name\n",
            "image/object/num float_list {\n",
            "  value: 14.0\n",
            "}\n",
            "\n",
            "image/view_hierarchy/bbox/xmin\n",
            "image/id bytes_list {\n",
            "  value: \"23860\"\n",
            "}\n",
            "\n",
            "image/ref_exp/text bytes_list {\n",
            "  value: \"click on the search icon\"\n",
            "}\n",
            "\n",
            "image/view_hierarchy/description bytes_list {\n",
            "  value: \"\"\n",
            "  value: \"\"\n",
            "  value: \"\"\n",
            "  value: \"\"\n",
            "  value: \"\"\n",
            "  value: \"\"\n",
            "  value: \"\"\n",
            "  value: \"\"\n",
            "  value: \"\"\n",
            "  value: \"\"\n",
            "  value: \"\"\n",
            "  value: \"\"\n",
            "  value: \"\"\n",
            "  value: \"\"\n",
            "  value: \"\"\n",
            "  value: \"\"\n",
            "  value: \"\"\n",
            "  value: \"\"\n",
            "  value: \"\"\n",
            "  value: \"\"\n",
            "  value: \"\"\n",
            "  value: \"\"\n",
            "  value: \"\"\n",
            "  value: \"\"\n",
            "  value: \"\"\n",
            "  value: \"\"\n",
            "  value: \"\"\n",
            "  value: \"\"\n",
            "  value: \"menu\"\n",
            "  value: \"\"\n",
            "  value: \"\"\n",
            "}\n",
            "\n",
            "image/view_hierarchy/bbox/ymax\n",
            "image/object/bbox/ymax\n",
            "image/view_hierarchy/id/name\n",
            "image/object/bbox/xmax\n",
            "image/view_hierarchy/class/label\n",
            "image/view_hierarchy/text bytes_list {\n",
            "  value: \"share to\"\n",
            "  value: \"\"\n",
            "  value: \"facebook\"\n",
            "  value: \"post your pinnatta on facebook\"\n",
            "  value: \"send to\"\n",
            "  value: \"\"\n",
            "  value: \"email\"\n",
            "  value: \"recipients will have to be selected via email\"\n",
            "  value: \"\"\n",
            "  value: \"\"\n",
            "  value: \"text sms\"\n",
            "  value: \"recipients will have to be selected via text sms\"\n",
            "  value: \"\"\n",
            "  value: \"\"\n",
            "  value: \"copy to clipboard\"\n",
            "  value: \"your pinnatta can be pasted to any app\"\n",
            "  value: \"pinnatta friends\"\n",
            "  value: \"\"\n",
            "  value: \"\"\n",
            "  value: \"the pinnatta team send us something to test the app\"\n",
            "  value: \"\"\n",
            "  value: \"\"\n",
            "  value: \"\"\n",
            "  value: \"konie pinnatta ask me anything\"\n",
            "  value: \"\"\n",
            "  value: \"\"\n",
            "  value: \"\"\n",
            "  value: \"\"\n",
            "  value: \"\"\n",
            "  value: \"\"\n",
            "  value: \"\"\n",
            "}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "record_count = 0\n",
        "for raw_record in raw_dataset:\n",
        "  record_count += 1\n",
        "\n",
        "print(f'Total records in the dataset: {record_count}')\n",
        "\n",
        "for raw_record in raw_dataset.take(3):\n",
        "  example = tf.train.Example()\n",
        "  example.ParseFromString(raw_record.numpy())\n",
        "  print('---------------------------------------------')\n",
        "  for key, feature in example.features.feature.items():\n",
        "    if key in [\"image/ref_exp/text\", \"image/id\", \"image/object/num\", \"image/view_hierarchy/description\", \"image/view_hierarchy/text\"] :\n",
        "      print(key, feature)\n",
        "    else:\n",
        "          print(key)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhnZZmhm1miG"
      },
      "source": [
        "That returns a `tf.train.Example` proto which is dificult to use as is, but it's fundamentally a representation of a:\n",
        "\n",
        "```\n",
        "Dict[str,\n",
        "     Union[List[float],\n",
        "           List[int],\n",
        "           List[str]]]\n",
        "```\n",
        "\n",
        "The following code manually converts the `Example` to a dictionary of NumPy arrays, without using TensorFlow Ops. Refer to [the PROTO file](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/example/feature.proto) for details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ziv9tiNE1l6J"
      },
      "outputs": [],
      "source": [
        "result = {}\n",
        "# example.features.feature is the dictionary\n",
        "for key, feature in example.features.feature.items():\n",
        "  # The values are the Feature objects which contain a `kind` which contains:\n",
        "  # one of three fields: bytes_list, float_list, int64_list\n",
        "\n",
        "  kind = feature.WhichOneof('kind')\n",
        "  result[key] = np.array(getattr(feature, kind).value)\n",
        "\n",
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0tFDrwdoj3q"
      },
      "source": [
        "## Walkthrough: Reading and writing image data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjN2LFxFpcR9"
      },
      "source": [
        "This is an end-to-end example of how to read and write image data using TFRecords. Using an image as input data, you will write the data as a TFRecord file, then read the file back and display the image.\n",
        "\n",
        "This can be useful if, for example, you want to use several models on the same input dataset. Instead of storing the image data raw, it can be preprocessed into the TFRecords format, and that can be used in all further processing and modelling.\n",
        "\n",
        "First, let's download [this image](https://commons.wikimedia.org/wiki/File:Felis_catus-cat_on_snow.jpg) of a cat in the snow and [this photo](https://upload.wikimedia.org/wikipedia/commons/f/fe/New_East_River_Bridge_from_Brooklyn_det.4a09796u.jpg) of the Williamsburg Bridge, NYC under construction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Lk2qrKvN0yu"
      },
      "source": [
        "### Fetch the images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3a0fmwg8lHdF"
      },
      "outputs": [],
      "source": [
        "cat_in_snow  = tf.keras.utils.get_file(\n",
        "    '320px-Felis_catus-cat_on_snow.jpg',\n",
        "    'https://storage.googleapis.com/download.tensorflow.org/example_images/320px-Felis_catus-cat_on_snow.jpg')\n",
        "\n",
        "williamsburg_bridge = tf.keras.utils.get_file(\n",
        "    '194px-New_East_River_Bridge_from_Brooklyn_det.4a09796u.jpg',\n",
        "    'https://storage.googleapis.com/download.tensorflow.org/example_images/194px-New_East_River_Bridge_from_Brooklyn_det.4a09796u.jpg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7aJJh7vENeE4"
      },
      "outputs": [],
      "source": [
        "display.display(display.Image(filename=cat_in_snow))\n",
        "display.display(display.HTML('Image cc-by: <a \"href=https://commons.wikimedia.org/wiki/File:Felis_catus-cat_on_snow.jpg\">Von.grzanka</a>'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KkW0uuhcXZqA"
      },
      "outputs": [],
      "source": [
        "display.display(display.Image(filename=williamsburg_bridge))\n",
        "display.display(display.HTML('<a \"href=https://commons.wikimedia.org/wiki/File:New_East_River_Bridge_from_Brooklyn_det.4a09796u.jpg\">From Wikimedia</a>'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSOgJSwoN5TQ"
      },
      "source": [
        "### Write the TFRecord file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Azx83ryQEU6T"
      },
      "source": [
        "As before, encode the features as types compatible with `tf.train.Example`. This stores the raw image string feature, as well as the height, width, depth, and arbitrary `label` feature. The latter is used when you write the file to distinguish between the cat image and the bridge image. Use `0` for the cat image, and `1` for the bridge image:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kC4TS1ZEONHr"
      },
      "outputs": [],
      "source": [
        "image_labels = {\n",
        "    cat_in_snow : 0,\n",
        "    williamsburg_bridge : 1,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c5njMSYNEhNZ"
      },
      "outputs": [],
      "source": [
        "# This is an example, just using the cat image.\n",
        "image_string = open(cat_in_snow, 'rb').read()\n",
        "\n",
        "label = image_labels[cat_in_snow]\n",
        "\n",
        "# Create a dictionary with features that may be relevant.\n",
        "def image_example(image_string, label):\n",
        "  image_shape = tf.io.decode_jpeg(image_string).shape\n",
        "\n",
        "  feature = {\n",
        "      'height': _int64_feature(image_shape[0]),\n",
        "      'width': _int64_feature(image_shape[1]),\n",
        "      'depth': _int64_feature(image_shape[2]),\n",
        "      'label': _int64_feature(label),\n",
        "      'image_raw': _bytes_feature(image_string),\n",
        "  }\n",
        "\n",
        "  return tf.train.Example(features=tf.train.Features(feature=feature))\n",
        "\n",
        "for line in str(image_example(image_string, label)).split('\\n')[:15]:\n",
        "  print(line)\n",
        "print('...')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2G_o3O9MN0Qx"
      },
      "source": [
        "Notice that all of the features are now stored in the `tf.train.Example` message. Next, functionalize the code above and write the example messages to a file named `images.tfrecords`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qcw06lQCOCZU"
      },
      "outputs": [],
      "source": [
        "# Write the raw image files to `images.tfrecords`.\n",
        "# First, process the two images into `tf.train.Example` messages.\n",
        "# Then, write to a `.tfrecords` file.\n",
        "record_file = 'images.tfrecords'\n",
        "with tf.io.TFRecordWriter(record_file) as writer:\n",
        "  for filename, label in image_labels.items():\n",
        "    image_string = open(filename, 'rb').read()\n",
        "    tf_example = image_example(image_string, label)\n",
        "    writer.write(tf_example.SerializeToString())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJrTe6tHPCfs"
      },
      "outputs": [],
      "source": [
        "!du -sh {record_file}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJSsCkZLPH6K"
      },
      "source": [
        "### Read the TFRecord file\n",
        "\n",
        "You now have the file—`images.tfrecords`—and can now iterate over the records in it to read back what you wrote. Given that in this example you will only reproduce the image, the only feature you will need is the raw image string. Extract it using the getters described above, namely `example.features.feature['image_raw'].bytes_list.value[0]`. You can also use the labels to determine which record is the cat and which one is the bridge:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M6Cnfd3cTKHN"
      },
      "outputs": [],
      "source": [
        "raw_image_dataset = tf.data.TFRecordDataset('images.tfrecords')\n",
        "\n",
        "# Create a dictionary describing the features.\n",
        "image_feature_description = {\n",
        "    'height': tf.io.FixedLenFeature([], tf.int64),\n",
        "    'width': tf.io.FixedLenFeature([], tf.int64),\n",
        "    'depth': tf.io.FixedLenFeature([], tf.int64),\n",
        "    'label': tf.io.FixedLenFeature([], tf.int64),\n",
        "    'image_raw': tf.io.FixedLenFeature([], tf.string),\n",
        "}\n",
        "\n",
        "def _parse_image_function(example_proto):\n",
        "  # Parse the input tf.train.Example proto using the dictionary above.\n",
        "  return tf.io.parse_single_example(example_proto, image_feature_description)\n",
        "\n",
        "parsed_image_dataset = raw_image_dataset.map(_parse_image_function)\n",
        "parsed_image_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PEEFPk4NEg1"
      },
      "source": [
        "Recover the images from the TFRecord file:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZf8jOyEIjSF"
      },
      "outputs": [],
      "source": [
        "for image_features in parsed_image_dataset:\n",
        "  image_raw = image_features['image_raw'].numpy()\n",
        "  display.display(display.Image(data=image_raw))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "pL--_KGdYoBz",
        "VrdQHgvNijTi",
        "laKnw9F3hL-W",
        "o6qxofy89obI",
        "1FISEuz8ubu3",
        "6aV0GQhV8tmp",
        "CKn5uql2lAaN"
      ],
      "name": "tfrecord_export.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}